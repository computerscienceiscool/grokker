package grokker

import (
	"bytes"
	"context"
	"encoding/json"
	"html/template"
	"io"
	"io/ioutil"
	"math"
	"os"
	"sort"
	"strings"

	. "github.com/stevegt/goadapt"

	"github.com/fabiustech/openai"
	"github.com/fabiustech/openai/models"
	chat "github.com/sashabaranov/go-openai"
)

// Grokker is a library for analyzing a set of documents and asking
// questions about them using the OpenAI chat and embeddings APIs.
//
// It uses this algorithm (generated by ChatGPT):
//
// To use embeddings in conjunction with the OpenAI Chat API to
// analyze a document, you can follow these general steps:
//
// (1) Break up the document into smaller text chunks or passages,
// each with a length of up to 8192 tokens (the maximum input size for
// the text-embedding-ada-002 model used by the Embeddings API).
//
// (2) For each text chunk, generate an embedding using the
// openai.Embedding.create() function. Store the embeddings for each
// chunk in a data structure such as a list or dictionary.
//
// (3) Use the Chat API to ask questions about the document. To do
// this, you can use the openai.Completion.create() function,
// providing the text of the previous conversation as the prompt
// parameter.
//
// (4) When a question is asked, use the embeddings of the document
// chunks to find the most relevant passages for the question. You can
// use a similarity measure such as cosine similarity to compare the
// embeddings of the question and each document chunk, and return the
// chunks with the highest similarity scores.
//
// (5) Provide the most relevant document chunks to the
// openai.Completion.create() function as additional context for
// generating a response. This will allow the model to better
// understand the context of the question and generate a more relevant
// response.
//
// Repeat steps 3-5 for each question asked, updating the conversation
// prompt as needed.

// Document is a single document in a document repository.
type Document struct {
	// The path to the document file.
	Path string
}

// Chunk is a single chunk of text from a document.
type Chunk struct {
	// The document that this chunk is from.
	Document *Document
	// The text of the chunk.
	Text string
	// The embedding of the chunk.
	Embedding []float64
}

type Grokker struct {
	// The OpenAI API key to use.
	apiKey string
	// The root directory of the document repository.
	Root string
	// The list of documents in the database.
	Documents []*Document
	// The list of chunks in the database.
	Chunks []*Chunk
	// The maximum number of tokens to use for each document chunk.
	MaxChunkSize int
	// Approximate number of characters per token.
	// XXX replace with a real tokenizer.
	CharsPerToken float64
}

// New creates a new Grokker database.
func New() *Grokker {
	return &Grokker{
		MaxChunkSize:  2048 * 4.0,
		CharsPerToken: 4.0,
	}
}

// Load loads a Grokker database from an io.Reader.
func Load(r io.Reader) (g *Grokker, err error) {
	buf, err := ioutil.ReadAll(r)
	Ck(err)
	g = &Grokker{}
	err = json.Unmarshal(buf, g)
	return
}

// Save saves a Grokker database as json data in an io.Writer.
func (g *Grokker) Save(w io.Writer) (err error) {
	data, err := json.Marshal(g)
	Ck(err)
	_, err = w.Write(data)
	return
}

// AddDocument adds a document to the Grokker database. It creates the
// embeddings for the document and adds them to the database.
func (g *Grokker) AddDocument(path string) (err error) {
	Return(&err)
	doc := &Document{
		Path: path,
	}
	g.Documents = append(g.Documents, doc)
	chunks, err := g.chunks(doc)
	Ck(err)
	g.AddChunks(doc, chunks)
	return
}

// AddChunks adds a sloce of chunks to the Grokker database.
func (g *Grokker) AddChunks(doc *Document, texts []string) {
	// For each text chunk, generate an embedding using the
	// openai.Embedding.create() function. Store the embeddings for each
	// chunk in a data structure such as a list or dictionary.
	embeddings, err := g.CreateEmbeddings(texts)
	Ck(err)
	for i, text := range texts {
		chunk := &Chunk{
			Document:  doc,
			Text:      text,
			Embedding: embeddings[i],
		}
		g.Chunks = append(g.Chunks, chunk)
	}
}

// Embeddings returns the embeddings for a slice of text chunks.
func (g *Grokker) CreateEmbeddings(texts []string) (embeddings [][]float64, err error) {
	// use github.com/fabiustech/openai library
	// XXX move client to Grokker struct
	token := os.Getenv("OPENAI_API_KEY")
	var c = openai.NewClient(token)
	// only send 100 chunks at a time
	for i := 0; i < len(texts); i += 100 {
		end := i + 100
		if end > len(texts) {
			end = len(texts)
		}
		req := &openai.EmbeddingRequest{
			Input: texts[i:end],
			Model: models.AdaEmbeddingV2,
		}
		res, err := c.CreateEmbeddings(context.Background(), req)
		Ck(err)
		for _, em := range res.Data {
			embeddings = append(embeddings, em.Embedding)
		}
	}
	return
}

// chunks returns a slice containing the chunks for a document.
func (g *Grokker) chunks(doc *Document) (c []string, err error) {
	Return(&err)
	// Break up the document into smaller text chunks or passages,
	// each with a length of up to 8192 tokens (the maximum input size for
	// the text-embedding-ada-002 model used by the Embeddings API).
	txt, err := ioutil.ReadFile(doc.Path)
	Ck(err)
	// for now, just split on paragraph boundaries
	paragraphs := strings.Split(string(txt), "\n\n")
	for _, paragraph := range paragraphs {
		// split the paragraph into chunks if it's too long.
		// XXX replace with a real tokenizer.
		for len(paragraph) > 0 {
			if len(paragraph) > g.MaxChunkSize {
				c = append(c, paragraph[:g.MaxChunkSize])
				paragraph = paragraph[g.MaxChunkSize:]
			} else {
				c = append(c, paragraph)
				paragraph = ""
			}
		}
	}
	return
}

// (4) When a question is asked, use the embeddings of the document
// chunks to find the most relevant passages for the question. You can
// use a similarity measure such as cosine similarity to compare the
// embeddings of the question and each document chunk, and return the
// chunks with the highest similarity scores.

// FindChunks returns the K most relevant chunks for a query.
func (g *Grokker) FindChunks(query string, K int) (chunks []*Chunk, err error) {
	Return(&err)
	// get the embeddings for the query.
	embeddings, err := g.CreateEmbeddings([]string{query})
	Ck(err)
	queryEmbedding := embeddings[0]
	// find the most similar chunks.
	chunks = g.SimilarChunks(queryEmbedding, K)
	return
}

// SimilarChunks returns the K most similar chunks to an embedding.
// If K is 0, it returns all chunks.
func (g *Grokker) SimilarChunks(embedding []float64, K int) (chunks []*Chunk) {
	// find the most similar chunks.
	type Sim struct {
		chunk *Chunk
		score float64
	}
	sims := make([]Sim, 0, len(g.Chunks))
	for _, chunk := range g.Chunks {
		score := Similarity(embedding, chunk.Embedding)
		sims = append(sims, Sim{chunk, score})
	}
	// sort the chunks by similarity.
	sort.Slice(sims, func(i, j int) bool {
		return sims[i].score > sims[j].score
	})
	// return the top K chunks.
	if K == 0 {
		K = len(sims)
	}
	for i := 0; i < K && i < len(sims); i++ {
		chunks = append(chunks, sims[i].chunk)
	}
	return
}

// Similarity returns the cosine similarity between two embeddings.
func Similarity(a, b []float64) float64 {
	var dot, magA, magB float64
	for i := range a {
		dot += a[i] * b[i]
		magA += a[i] * a[i]
		magB += b[i] * b[i]
	}
	return dot / (math.Sqrt(magA) * math.Sqrt(magB))
}

// (5) Provide the most relevant document chunks to the
// openai.Completion.create() function as additional context for
// generating a response. This will allow the model to better
// understand the context of the question and generate a more relevant
// response.

// Answer returns the answer to a question.
func (g *Grokker) Answer(question string, global bool) (resp chat.ChatCompletionResponse, query string, err error) {
	Return(&err)
	// get all chunks, sorted by similarity to the question.
	chunks, err := g.FindChunks(question, 0)
	Ck(err)
	// use chunks as context for the answer until we reach the max context size.
	var context string
	for _, chunk := range chunks {
		context += chunk.Text + "\n\n"
		if len(context)+len(promptTmpl) > g.MaxChunkSize {
			break
		}
	}

	// generate the answer.
	resp, query, err = g.Generate(question, context, global)
	return
}

// Use the openai.Completion.create() function to generate a
// response to the question. You can use the prompt parameter to
// provide the question, and the max_tokens parameter to limit the
// length of the response.

// var promptTmpl = `You are a helpful assistant.  Answer the following question and summarize the context:
// var promptTmpl = `You are a helpful assistant.
var promptTmpl = `{{.Question}}

Context:
{{.Context}}`

var XXXpromptTmpl = `{{.Question}}`

// Generate returns the answer to a question.
func (g *Grokker) Generate(question, ctxt string, global bool) (resp chat.ChatCompletionResponse, query string, err error) {
	Return(&err)

	// use 	"github.com/sashabaranov/go-openai"
	// XXX move client to Grokker struct
	token := os.Getenv("OPENAI_API_KEY")
	client := chat.NewClient(token)

	if global {
		// first get an answer from the model
		resp, err = client.CreateChatCompletion(
			context.Background(),
			chat.ChatCompletionRequest{
				Model: chat.GPT3Dot5Turbo,
				Messages: []chat.ChatCompletionMessage{
					{
						Role:    chat.ChatMessageRoleSystem,
						Content: "You are a helpful assistant.",
					},
					{
						Role:    chat.ChatMessageRoleUser,
						Content: question,
					},
				},
			},
		)
		// insert the answer at the beginning of the context
		// XXX this needs to be done earlier so we don't exceed max tokens
		ctxt = resp.Choices[0].Message.Content + "\n\n" + ctxt
	}

	// process template
	t := template.Must(template.New("prompt").Parse(promptTmpl))
	var buf bytes.Buffer
	err = t.Execute(&buf, struct {
		Question string
		Context  string
	}{
		Question: question,
		Context:  ctxt,
	})
	query = buf.String()
	// Pf("query:\n\n%s\n", query)

	// now get the answer with the context
	resp, err = client.CreateChatCompletion(
		context.Background(),
		chat.ChatCompletionRequest{
			Model: chat.GPT3Dot5Turbo,
			Messages: []chat.ChatCompletionMessage{
				{
					Role:    chat.ChatMessageRoleSystem,
					Content: "You are a helpful assistant.",
				},
				{
					Role:    chat.ChatMessageRoleUser,
					Content: query,
				},
			},
		},
	)
	Ck(err)

	// fmt.Println(resp.Choices[0].Message.Content)
	return
}
