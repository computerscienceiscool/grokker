package grokker

import (
	"context"
	"encoding/json"
	"fmt"
	"io"
	"io/ioutil"
	"math"
	"os"
	"sort"
	"strings"
	"time"

	. "github.com/stevegt/goadapt"

	"github.com/fabiustech/openai"
	fabius_models "github.com/fabiustech/openai/models"

	oai "github.com/sashabaranov/go-openai"
)

// Grokker is a library for analyzing a set of documents and asking
// questions about them using the OpenAI chat and embeddings APIs.
//
// It uses this algorithm (generated by ChatGPT):
//
// To use embeddings in conjunction with the OpenAI Chat API to
// analyze a document, you can follow these general steps:
//
// (1) Break up the document into smaller text chunks or passages,
// each with a length of up to 8192 tokens (the maximum input size for
// the text-embedding-ada-002 model used by the Embeddings API).
//
// (2) For each text chunk, generate an embedding using the
// openai.Embedding.create() function. Store the embeddings for each
// chunk in a data structure such as a list or dictionary.
//
// (3) Use the Chat API to ask questions about the document. To do
// this, you can use the openai.Completion.create() function,
// providing the text of the previous conversation as the prompt
// parameter.
//
// (4) When a question is asked, use the embeddings of the document
// chunks to find the most relevant passages for the question. You can
// use a similarity measure such as cosine similarity to compare the
// embeddings of the question and each document chunk, and return the
// chunks with the highest similarity scores.
//
// (5) Provide the most relevant document chunks to the
// openai.Completion.create() function as additional context for
// generating a response. This will allow the model to better
// understand the context of the question and generate a more relevant
// response.
//
// Repeat steps 3-5 for each question asked, updating the conversation
// prompt as needed.

// Model is a type for model name and characteristics
type Model struct {
	Name       string
	TokenLimit int
	oaiModel   string
	active     bool
}

func (m *Model) String() string {
	status := ""
	if m.active {
		status = "*"
	}
	return fmt.Sprintf("%1s %-20s tokens: %d)", status, m.Name, m.TokenLimit)
}

// Models is a type that manages the set of available models.
type Models struct {
	// The list of available models.
	Available map[string]*Model
}

// NewModels creates a new Models object.
func NewModels() (m *Models) {
	m = &Models{}
	m.Available = map[string]*Model{
		"gpt-3.5-turbo": {"", 4096, oai.GPT3Dot5Turbo, false},
		"gpt-4":         {"", 8192, oai.GPT4, false},
		"gpt-4-32k":     {"", 32768, oai.GPT432K, false}, // XXX deprecated in openai-go 1.9.0
		// "gpt-4-32k": {"", 32768, oai.GPT4_32K, false}, // XXX future version of openai-go
	}
	// fill in the model names
	for k, v := range m.Available {
		v.Name = k
		m.Available[k] = v
	}
	return
}

// ls returns a list of available models.
func (models *Models) ls() (list []*Model) {
	for _, v := range models.Available {
		list = append(list, v)
	}
	return
}

var DefaultModel = "gpt-3.5-turbo"

// findModel returns the model name and model_t given a model name.
// if the given model name is empty, then use DefaultModel.
func (models *Models) findModel(model string) (name string, m *Model, err error) {
	if model == "" {
		model = DefaultModel
	}
	m, ok := models.Available[model]
	if !ok {
		err = fmt.Errorf("model %q not found", model)
		return
	}
	name = model
	return
}

// Document is a single document in a document repository.
type Document struct {
	// The path to the document file.
	Path string
}

// Chunk is a single chunk of text from a document.
type Chunk struct {
	// The document that this chunk is from.
	// XXX this is redundant; we could just use the document's path.
	// XXX a chunk should be able to be from multiple documents.
	Document *Document
	// The text of the chunk.
	Text string
	// The embedding of the chunk.
	Embedding []float64
}

type Grokker struct {
	embeddingClient *openai.Client
	chatClient      *oai.Client
	// The root directory of the document repository.
	// XXX not used yet.
	Root string
	// The list of documents in the database.
	Documents []*Document
	// The list of chunks in the database.
	Chunks []*Chunk
	// model specs
	models   *Models
	Model    string
	oaiModel string
	// XXX use a real tokenizer and replace maxChunkLen with tokenLimit.
	// tokenLimit int
	maxChunkLen          int
	maxEmbeddingChunkLen int
}

// New creates a new Grokker database.
func New(model string) (g *Grokker, err error) {
	defer Return(&err)
	g = &Grokker{}
	err = g.initModel(model)
	Ck(err)
	g.initClients()
	return
}

// Load loads a Grokker database from an io.Reader.
func Load(r io.Reader) (g *Grokker, err error) {
	defer Return(&err)
	buf, err := ioutil.ReadAll(r)
	Ck(err)
	g = &Grokker{}
	err = json.Unmarshal(buf, g)
	Ck(err)
	err = g.initModel(g.Model)
	Ck(err)
	g.initClients()
	return
}

// initClients initializes the OpenAI clients.
func (g *Grokker) initClients() {
	authtoken := os.Getenv("OPENAI_API_KEY")
	g.embeddingClient = openai.NewClient(authtoken)
	g.chatClient = oai.NewClient(authtoken)
	return
}

// initModel initializes the model for a new or reloaded Grokker database.
func (g *Grokker) initModel(model string) (err error) {
	defer Return(&err)
	g.models = NewModels()
	model, m, err := g.models.findModel(model)
	Ck(err)
	m.active = true
	g.Model = model
	g.oaiModel = m.oaiModel
	// XXX replace with a real tokenizer.
	charsPerToken := 3.1
	g.maxChunkLen = int(math.Floor(float64(m.TokenLimit) * charsPerToken))
	// XXX replace with a real tokenizer.
	// XXX hardcoded for the text-embedding-ada-002 model
	g.maxEmbeddingChunkLen = int(math.Floor(float64(8192) * charsPerToken))
	return
}

// UpgradeModel upgrades the model for a Grokker database.
func (g *Grokker) UpgradeModel(model string) (err error) {
	defer Return(&err)
	model, m, err := g.models.findModel(model)
	Ck(err)
	oldModel, oldM, err := g.getModel()
	Ck(err)
	// allow upgrade to a larger model, but not a smaller one
	if m.TokenLimit < oldM.TokenLimit {
		err = fmt.Errorf("cannot downgrade model from '%s' to '%s'", oldModel, model)
		return
	}
	g.initModel(model)
	return
}

// getModel returns the current model name and model_t from the db
func (g *Grokker) getModel() (model string, m *Model, err error) {
	defer Return(&err)
	model, m, err = g.models.findModel(g.Model)
	Ck(err)
	return
}

// Save saves a Grokker database as json data in an io.Writer.
func (g *Grokker) Save(w io.Writer) (err error) {
	defer Return(&err)
	data, err := json.Marshal(g)
	Ck(err)
	_, err = w.Write(data)
	return
}

// UpdateEmbeddings updates the embeddings for any documents that have
// changed since the last time the embeddings were updated.  It returns
// true if any embeddings were updated.
func (g *Grokker) UpdateEmbeddings(lastUpdate time.Time) (update bool, err error) {
	defer Return(&err)
	// we use the timestamp of the grokfn as the last embedding update time.
	for _, doc := range g.Documents {
		// check if the document has changed.
		fi, err := os.Stat(doc.Path)
		if os.IsNotExist(err) {
			// document has been removed; remove it from the database.
			g.RemoveDocument(doc)
			update = true
			continue
		}
		Ck(err)
		if fi.ModTime().After(lastUpdate) {
			// update the embeddings.
			Debug("updating embeddings for %s ...", doc.Path)
			updated, err := g.UpdateDocument(doc)
			Ck(err)
			Debug("done\n")
			update = update || updated
		}
	}
	// garbage collect any chunks that are no longer referenced.
	g.GC()
	return
}

// AddDocument adds a document to the Grokker database. It creates the
// embeddings for the document and adds them to the database.
func (g *Grokker) AddDocument(path string) (err error) {
	defer Return(&err)
	doc := &Document{
		Path: path,
	}
	// find out if the document is already in the database.
	found := false
	for _, d := range g.Documents {
		if d.Path == doc.Path {
			found = true
			break
		}
	}
	if !found {
		// add the document to the database.
		g.Documents = append(g.Documents, doc)
	}
	// update the embeddings for the document.
	_, err = g.UpdateDocument(doc)
	Ck(err)
	return
}

// RemoveDocument removes a document from the Grokker database.
func (g *Grokker) RemoveDocument(doc *Document) (err error) {
	defer Return(&err)
	Debug("removing document %s ...", doc.Path)
	// remove the document from the database.
	for i, d := range g.Documents {
		if d.Path == doc.Path {
			g.Documents = append(g.Documents[:i], g.Documents[i+1:]...)
			break
		}
	}
	// the document chunks are still in the database, but they will be
	// removed during garbage collection.
	return
}

// GC removes any chunks that are no longer referenced by any document.
func (g *Grokker) GC() (err error) {
	defer Return(&err)
	// for each chunk, check if it is referenced by any document.
	// if not, remove it from the database.
	oldLen := len(g.Chunks)
	newChunks := make([]*Chunk, 0, len(g.Chunks))
	for _, chunk := range g.Chunks {
		// check if the chunk is referenced by any document.
		referenced := false
		for _, doc := range g.Documents {
			if doc.Path == chunk.Document.Path {
				referenced = true
				break
			}
		}
		if referenced {
			newChunks = append(newChunks, chunk)
		}
	}
	g.Chunks = newChunks
	newLen := len(g.Chunks)
	Debug("garbage collected %d chunks from the database", oldLen-newLen)
	return
}

// UpdateDocument updates the embeddings for a document and returns
// true if the document was updated.
func (g *Grokker) UpdateDocument(doc *Document) (updated bool, err error) {
	defer Return(&err)
	// XXX much of this code is inefficient and will be replaced
	// when we have a kv store.
	defer Return(&err)
	Debug("updating embeddings for %s ...", doc.Path)
	// break the doc up into chunks.
	chunkStrings, err := g.chunkStrings(doc)
	Ck(err)
	// get a list of the existing chunks for this document.
	var oldChunks []*Chunk
	var newChunkStrings []string
	for _, chunk := range g.Chunks {
		if chunk.Document.Path == doc.Path {
			oldChunks = append(oldChunks, chunk)
		}
	}
	Debug("found %d existing chunks", len(oldChunks))
	// for each chunk, check if it already exists in the database.
	for _, chunkString := range chunkStrings {
		found := false
		for _, oldChunk := range oldChunks {
			if oldChunk.Text == chunkString {
				// the chunk already exists in the database.  remove it from the list of old chunks.
				found = true
				for i, c := range oldChunks {
					if c == oldChunk {
						oldChunks = append(oldChunks[:i], oldChunks[i+1:]...)
						break
					}
				}
				break
			}
		}
		if !found {
			// the chunk does not exist in the database.  add it.
			updated = true
			newChunkStrings = append(newChunkStrings, chunkString)
		}
	}
	Debug("found %d new chunks", len(newChunkStrings))
	// orphaned chunks will be garbage collected.

	// For each text chunk, generate an embedding using the
	// openai.Embedding.create() function. Store the embeddings for each
	// chunk in a data structure such as a list or dictionary.
	embeddings, err := g.CreateEmbeddings(newChunkStrings)
	Ck(err)
	for i, text := range newChunkStrings {
		chunk := &Chunk{
			Document:  doc,
			Text:      text,
			Embedding: embeddings[i],
		}
		g.Chunks = append(g.Chunks, chunk)
	}
	return
}

// Embeddings returns the embeddings for a slice of text chunks.
func (g *Grokker) CreateEmbeddings(texts []string) (embeddings [][]float64, err error) {
	// use github.com/fabiustech/openai library
	c := g.embeddingClient
	// simply return an empty list if there are no texts.
	if len(texts) == 0 {
		return
	}
	// iterate over the text chunks and create one or more embedding queries
	for i := 0; i < len(texts); {
		// add texts to the current query until we reach the token limit
		// XXX use a real tokenizer
		// i is the index of the first text in the current query
		// j is the index of the last text in the current query
		// XXX this is ugly, fragile, and needs to be tested and refactored
		totalLen := 0
		j := i
		for {
			nextLen := len(texts[j])
			Debug("i=%d j=%d nextLen=%d totalLen=%d", i, j, nextLen, totalLen)
			Assert(nextLen > 0)
			Assert(nextLen <= g.maxEmbeddingChunkLen, "nextLen=%d maxEmbeddingChunkLen=%d", nextLen, g.maxEmbeddingChunkLen)
			if totalLen+nextLen >= g.maxEmbeddingChunkLen {
				j--
				Debug("breaking because totalLen=%d nextLen=%d", totalLen, nextLen)
				break
			}
			totalLen += nextLen
			if j == len(texts)-1 {
				Debug("breaking because j=%d len(texts)=%d", j, len(texts))
				break
			}
			j++
		}
		Debug("i=%d j=%d totalLen=%d", i, j, totalLen)
		Assert(j >= i, "j=%d i=%d", j, i)
		Assert(totalLen > 0, "totalLen=%d", totalLen)
		inputs := texts[i : j+1]
		// double-check that the total length is within the limit and that
		// no individual text is too long.
		totalLen = 0
		for _, text := range inputs {
			totalLen += len(text)
			Debug("len(text)=%d, totalLen=%d", len(text), totalLen)
			Assert(len(text) <= g.maxEmbeddingChunkLen, "text too long: %d", len(text))
		}
		Assert(totalLen <= g.maxEmbeddingChunkLen, "totalLen=%d maxEmbeddingChunkLen=%d", totalLen, g.maxEmbeddingChunkLen)
		req := &openai.EmbeddingRequest{
			Input: inputs,
			Model: fabius_models.AdaEmbeddingV2,
		}
		res, err := c.CreateEmbeddings(context.Background(), req)
		Ck(err)
		for _, em := range res.Data {
			embeddings = append(embeddings, em.Embedding)
		}
		i = j + 1
	}
	Debug("created %d embeddings", len(embeddings))
	Assert(len(embeddings) == len(texts))
	return
}

// chunkStrings returns a slice containing the chunk strings for a document.
func (g *Grokker) chunkStrings(doc *Document) (c []string, err error) {
	defer Return(&err)
	// read the document.
	txt, err := ioutil.ReadFile(doc.Path)
	Ck(err)
	return g.chunks(string(txt)), nil
}

// chunks returns a slice containing the chunk strings for a string.
func (g *Grokker) chunks(txt string) (c []string) {
	// Break up the text into smaller text chunks or passages, each
	// with a length of up to the limit for the model used by the
	// Embeddings API
	//
	// XXX splitting on paragraphs is not ideal.  smarter splitting
	// might look at the structure of the text and split on
	// sections, chapters, etc.  it might also be useful to include
	// metadata such as file names.
	paragraphs := strings.Split(string(txt), "\n\n")
	for _, paragraph := range paragraphs {
		// split the paragraph into chunks if it's too long.
		// XXX replace with a real tokenizer.
		for len(paragraph) > 0 {
			if len(paragraph) >= g.maxEmbeddingChunkLen {
				split := g.maxEmbeddingChunkLen - 1
				c = append(c, paragraph[:split])
				paragraph = paragraph[split:]
			} else {
				c = append(c, paragraph)
				paragraph = ""
			}
		}
	}
	return
}

// (4) When a question is asked, use the embeddings of the document
// chunks to find the most relevant passages for the question. You can
// use a similarity measure such as cosine similarity to compare the
// embeddings of the question and each document chunk, and return the
// chunks with the highest similarity scores.

// FindChunks returns the K most relevant chunks for a query.
func (g *Grokker) FindChunks(query string, K int) (chunks []*Chunk, err error) {
	defer Return(&err)
	// get the embeddings for the query.
	embeddings, err := g.CreateEmbeddings([]string{query})
	Ck(err)
	queryEmbedding := embeddings[0]
	// find the most similar chunks.
	chunks = g.SimilarChunks(queryEmbedding, K)
	return
}

// SimilarChunks returns the K most similar chunks to an embedding.
// If K is 0, it returns all chunks.
func (g *Grokker) SimilarChunks(embedding []float64, K int) (chunks []*Chunk) {
	Debug("chunks in database: %d", len(g.Chunks))
	// find the most similar chunks.
	type Sim struct {
		chunk *Chunk
		score float64
	}
	sims := make([]Sim, 0, len(g.Chunks))
	for _, chunk := range g.Chunks {
		score := Similarity(embedding, chunk.Embedding)
		sims = append(sims, Sim{chunk, score})
	}
	// sort the chunks by similarity.
	sort.Slice(sims, func(i, j int) bool {
		return sims[i].score > sims[j].score
	})
	// return the top K chunks.
	if K == 0 {
		K = len(sims)
	}
	for i := 0; i < K && i < len(sims); i++ {
		chunks = append(chunks, sims[i].chunk)
	}
	Debug("found %d similar chunks", len(chunks))
	return
}

// Similarity returns the cosine similarity between two embeddings.
func Similarity(a, b []float64) float64 {
	var dot, magA, magB float64
	for i := range a {
		dot += a[i] * b[i]
		magA += a[i] * a[i]
		magB += b[i] * b[i]
	}
	return dot / (math.Sqrt(magA) * math.Sqrt(magB))
}

// (5) Provide the most relevant document chunks to the
// openai.Completion.create() function as additional context for
// generating a response. This will allow the model to better
// understand the context of the question and generate a more relevant
// response.

// Answer returns the answer to a question.
func (g *Grokker) Answer(question string, global bool) (resp oai.ChatCompletionResponse, query string, err error) {
	defer Return(&err)
	// get all chunks, sorted by similarity to the question.
	chunks, err := g.FindChunks(question, 0)
	Ck(err)
	// ensure the context is not too long.
	maxSize := int(float64(g.maxChunkLen)*0.5) - len(question)
	// use chunks as context for the answer until we reach the max size.
	var context string
	for _, chunk := range chunks {
		context += chunk.Text + "\n\n"
		if len(context)+len(promptTmpl) > maxSize {
			break
		}
	}
	Debug("using %d chunks as context", len(chunks))

	// generate the answer.
	resp, query, err = g.Generate(question, context, global)
	return
}

// Use the openai.Completion.create() function to generate a
// response to the question. You can use the prompt parameter to
// provide the question, and the max_tokens parameter to limit the
// length of the response.

// var promptTmpl = `You are a helpful assistant.  Answer the following question and summarize the context:
// var promptTmpl = `You are a helpful assistant.
var promptTmpl = `{{.Question}}

Context:
{{.Context}}`

var XXXpromptTmpl = `{{.Question}}`

// Generate returns the answer to a question.
func (g *Grokker) Generate(question, ctxt string, global bool) (resp oai.ChatCompletionResponse, query string, err error) {
	defer Return(&err)

	/*
		var systemText string
		if global {
			systemText = "You are a helpful assistant that provides answers from everything you know, as well as from the context provided in this chat."
		} else {
			systemText = "You are a helpful assistant that provides answers from the context provided in this chat."
		}
	*/

	// XXX don't exceed max tokens
	messages := []oai.ChatCompletionMessage{
		{
			Role:    oai.ChatMessageRoleSystem,
			Content: "You are a helpful assistant.",
		},
	}

	// first get global knowledge
	if global {
		messages = append(messages, oai.ChatCompletionMessage{
			Role:    oai.ChatMessageRoleUser,
			Content: question,
		})
		resp, err = g.chat(messages)
		Ck(err)
		// add the response to the messages.
		messages = append(messages, oai.ChatCompletionMessage{
			Role:    oai.ChatMessageRoleAssistant,
			Content: resp.Choices[0].Message.Content,
		})
	}

	// add context from local sources
	if len(ctxt) > 0 {
		messages = append(messages, []oai.ChatCompletionMessage{
			{
				Role:    oai.ChatMessageRoleUser,
				Content: ctxt,
			},
			{
				Role:    oai.ChatMessageRoleAssistant,
				Content: "Great! I've read the context.",
			},
		}...)
	}

	// now ask the question
	messages = append(messages, oai.ChatCompletionMessage{
		Role:    oai.ChatMessageRoleUser,
		Content: question,
	})

	// get the answer
	resp, err = g.chat(messages)

	// fmt.Println(resp.Choices[0].Message.Content)
	// Pprint(messages)
	// Pprint(resp)
	return
}

// chat uses the openai API to continue a conversation given a
// (possibly synthesized) message history.
func (g *Grokker) chat(messages []oai.ChatCompletionMessage) (resp oai.ChatCompletionResponse, err error) {
	defer Return(&err)

	model := g.oaiModel
	Debug("chat model: %s", model)
	Debug("chat: messages: %v", messages)

	// use 	"github.com/sashabaranov/go-openai"
	client := g.chatClient
	resp, err = client.CreateChatCompletion(
		context.Background(),
		oai.ChatCompletionRequest{
			Model:    model,
			Messages: messages,
		},
	)
	Ck(err, "%#v", messages)
	totalBytes := 0
	for _, msg := range messages {
		totalBytes += len(msg.Content)
	}
	totalBytes += len(resp.Choices[0].Message.Content)
	ratio := float64(totalBytes) / float64(resp.Usage.TotalTokens)
	Debug("total tokens: %d  char/token ratio: %.1f\n", resp.Usage.TotalTokens, ratio)
	return
}

// ListModels lists the available models.
func (g *Grokker) ListModels() (models []*Model, err error) {
	defer Return(&err)
	for _, model := range g.models.Available {
		models = append(models, model)
	}
	return
}

// RefreshEmbeddings refreshes the embeddings for all documents in the
// database.
func (g *Grokker) RefreshEmbeddings() (err error) {
	defer Return(&err)
	// regenerate the embeddings for each document.
	for _, doc := range g.Documents {
		// remove file from list if it doesn't exist.
		_, err := os.Stat(doc.Path)
		if os.IsNotExist(err) {
			// remove the document from the database.
			g.RemoveDocument(doc)
			continue
		}
		Ck(err)
		_, err = g.UpdateDocument(doc)
		Ck(err)
	}
	g.GC()
	return
}

var GitCommitPrompt = `
Use the information in the following diff to write a concise git commit message and nothing else. The first line of the commit message must be a summary of no more than 60 characters that briefly describes the changes in the diff. Use bullet points to describe the changes made. Use present tense. 
`

// GitCommitMessage generates a git commit message given a diff. It
// appends a reasonable prompt, and then uses the result as a grokker
// query.
func (g *Grokker) GitCommitMessage(diff string) (resp oai.ChatCompletionResponse, query string, err error) {
	defer Return(&err)

	// format the prompt
	prompt := Spf("%s\n%s", GitCommitPrompt, diff)

	// ensure the prompt is not too long
	if len(prompt) > int(float64(g.maxChunkLen)*.7) {
		err = fmt.Errorf("diff is too long -- try unstaging some files")
		return
	}

	// use the result as a grokker query
	resp, query, err = g.Answer(prompt, false)
	// resp, _, err = g.Generate(GitCommitPrompt, diff, false)
	Ck(err)
	return
}

// summarizeDiff summarizes a diff.
func (g *Grokker) summarizeDiff(diff string) (summary string, err error) {
	defer Return(&err)
	maxSize := int(float64(g.maxChunkLen) * 0.5)
	// split the diff on filenames
	chunks := strings.Split(diff, "diff --git")
	// summarize each file's changes
	for _, chunk := range chunks {
		// format the chunk
		context := Spf("diff --git%s", chunk)
		if len(context) > maxSize {
			context = context[:maxSize]
		}
		question := "summarize the diff"
		resp, _, err := g.Generate(question, context, false)
		Ck(err)
		summary = Spf("%s\n\n%s", summary, resp.Choices[0].Message.Content)
	}
	return
}
